{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3d8279b",
   "metadata": {},
   "source": [
    "#### Gradients are essesntial for our model optimization. Pytorch provides the Autograd package which can do all the computations for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceaabfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a0947a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0197,  0.9841, -0.2759], requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc9069a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16bf2fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.9803, 2.9841, 1.7241], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f32cce",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li>Our input is \"x\" and \"2\" and our output is \"y\". our operation is \"+\".</li>\n",
    "    <li>Now with the technique \"Backpropagation\",we can calculate the gradients</li>\n",
    "    <li>So, First we do a forward pass and apply the \"+\" operation and calculate the output \"y\".</li>\n",
    "    <li>And since we specified that it requires the gradient(requires_grad), Pytorch will automatically create and store a function for us.</li>\n",
    "    <li>This function is then used in backpropagation and to get the gradients.</li>\n",
    "    <li>So, the \"y\" ill have an attribute \"grad_fn\" which will point to a gradient function and in this case, it's called \"Add Backward\"</li>\n",
    "    <li>So, this function will caculate gradient of y w.r.t \"x\" (dy/dx)</li>\n",
    "    <li>Now we see \"grad_fn\" while printing \"y\"</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af9cbfb",
   "metadata": {},
   "source": [
    "<h3>Backpropagation</h3>\n",
    "<p>Backpropagation is the algorithm of calculating the gradients of the cost function with respect to the weights. Backpropagation is used to improve the output of neural networks. It does this by propagating the error in a backward direction and calculating the gradient of the cost function for each weight.</p>\n",
    "It is the practice of fine-tuning the weights of a neural net based on the error rate (i.e. loss) obtained in the previous epoch (i.e. iteration.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05bc6c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7.8434, 17.8091,  5.9450], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = y*y*2\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41eeb10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.5325, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_mean = z.mean()\n",
    "z_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fb1ba2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.6404, 3.9787, 2.2988])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To calculate gradient\n",
    "z_mean.backward(retain_graph=True) #dz/dx #retain_graph=True---> Retain the computational graph for calling .backward() next time in future\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d6b48a",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li>In the background, what this does is create a vector jacobian product to get gradients</li>\n",
    "    <li>So, we have the jacobian matrix with partial derivates and then we multiply this with a gradient vector and then we will get the final gradients we are interested in</li>\n",
    "    <li>This is also called chain rule</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d05b2d8",
   "metadata": {},
   "source": [
    "#### Note: backward() will throw error if requires_grad=False, so keep it true to calculate gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05cd42b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7.8434, 17.8091,  5.9450], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dada4c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#z.backward() -> only using this throws error as grad(gradient) can be implicityly created only for scalar outputs.\n",
    "\n",
    "v = torch.tensor([0.1, 1.0, 0.001], dtype=torch.float32)\n",
    "#z.backward(v) #<--- In background, it is doin an vector jacobian product\n",
    "# z.backward(v) threw a below error because previously we did not use retain_graph=True parameter in z_mean.backward()\n",
    "\"\"\"\n",
    "RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already \n",
    "been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). \n",
    "Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved \n",
    "tensors after calling backward.\n",
    "\"\"\"\n",
    "z.backward(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4861a039",
   "metadata": {},
   "source": [
    "<h3>Here's the breakdown of the issue:</h3>\n",
    "\n",
    "<p>Calling z_mean.backward() calculates the gradients for the computation that resulted in z_mean.</p>\n",
    "<p>By default, PyTorch releases the intermediate values used during the calculation after the first backward pass.</p>\n",
    "<p>When you try z.backward(v), PyTorch tries to access these intermediate values again, but they are already freed, leading to the error \"Trying to backward through the graph a second time\".</p>\n",
    "<b>Solution: Use retain_graph=True</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b2eba2",
   "metadata": {},
   "source": [
    "<h3>How to prevent pytorch from tracking the history and calculating the grad_fn attribute.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232fe3f4",
   "metadata": {},
   "source": [
    "So, Sometimes during our training loop when we want to update our weights then this operation should not be a part of the gradient computation.\n",
    "\n",
    "There are 3 options to do so:\n",
    "1. x.requires_grad_(False) <-- inplace function\n",
    "2. x.detach() <-- This would create a new tensor that doesn't require the gradient\n",
    "3. with torhc.no_grad():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18dbd130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0197,  0.9841, -0.2759], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a9767f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x.requires_grad_(False)\n",
    "# output : tensor([-0.9846, -0.4701, -0.7996]) <-- values are random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e865fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0197,  0.9841, -0.2759])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f1b3adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.9803, 2.9841, 1.7241], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c22cfd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.9803, 2.9841, 1.7241])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y = x+2\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d30e5c",
   "metadata": {},
   "source": [
    "Whenever we call the backward function, the gradient for this tensor will be accumulated into the .grad attribute\n",
    "So, Their values will be summed up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10b6c3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(5):\n",
    "    model_output = (weights*3).sum() # dummpy operation which will simulate some model output\n",
    "    \n",
    "    #Calculate gradient\n",
    "    model_output.backward()\n",
    "    \n",
    "    print(weights.grad)\n",
    "    \n",
    "    # If we do more n iterations, the nth backward call will keep accumulating the values and write them into .grad attribute\n",
    "    \"\"\" n=5\n",
    "    tensor([3., 3., 3., 3.])\n",
    "    tensor([6., 6., 6., 6.])\n",
    "    tensor([9., 9., 9., 9.])\n",
    "    tensor([12., 12., 12., 12.])\n",
    "    tensor([15., 15., 15., 15.])\n",
    "    \"\"\"\n",
    "    \n",
    "    #To handle this, we must empty the gradients\n",
    "    weights.grad.zero_()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7a9d892",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.SGD([weights], lr=0.01)\n",
    "optimizer.step()\n",
    "optimizer.zero_grad() #<--empties the grad like above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dd9233",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d584643",
   "metadata": {},
   "source": [
    "#### requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398b78d9",
   "metadata": {},
   "source": [
    "In PyTorch, the requires_grad attribute is a flag that is used to indicate whether or not gradients should be computed for a tensor during backpropagation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2a19f1",
   "metadata": {},
   "source": [
    "1. When set to True for a tensor, PyTorch tracks the operations performed on that tensor during the forward pass of your computational graph.\n",
    "\n",
    "2. This allows PyTorch to calculate the gradients of the loss function with respect to that tensor during the backward pass.\n",
    "\n",
    "3. Gradients are essential for training neural networks, as they indicate how adjustments to the tensor's values can improve the model's performance.\n",
    "\n",
    "4. PyTorch dynamically builds a computational graph during the forward pass, tracking operations involving tensors with requires_grad=True. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f625939",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6982ded2",
   "metadata": {},
   "source": [
    "### Here's a simple example code in PyTorch demonstrating backpropagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1b3791c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients w.r.t. weights (w): tensor([ -7.4605, -14.9209, -22.3814])\n",
      "Gradient w.r.t. bias (b): tensor([-7.4605])\n"
     ]
    }
   ],
   "source": [
    "# Define input data and ground truth\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y_true = torch.tensor([2.0], dtype=torch.float32)\n",
    "\n",
    "# Initialize model parameters (weights and biases)\n",
    "w = torch.randn(3, requires_grad=True)  # Weights\n",
    "b = torch.randn(1, requires_grad=True)  # Bias\n",
    "\n",
    "# Define model prediction and loss function\n",
    "def model(x):\n",
    "    return torch.sum(w * x) + b  # Linear regression model\n",
    "\n",
    "def loss_fn(y_pred, y_true):\n",
    "    return torch.mean((y_pred - y_true) ** 2)  # Mean squared error loss\n",
    "\n",
    "# Forward pass\n",
    "y_pred = model(x)\n",
    "\n",
    "# Compute the loss\n",
    "loss = loss_fn(y_pred, y_true)\n",
    "\n",
    "# Backward pass (compute gradients)\n",
    "loss.backward()\n",
    "\n",
    "# Print gradients\n",
    "print(\"Gradients w.r.t. weights (w):\", w.grad)\n",
    "print(\"Gradient w.r.t. bias (b):\", b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b17756d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
